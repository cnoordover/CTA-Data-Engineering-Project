{
	"jobConfig": {
		"name": "publish job",
		"description": "",
		"role": "arn:aws:iam::471112705806:role/service-role/AWSGlueServiceRole",
		"command": "pythonshell",
		"version": "1.0",
		"runtime": null,
		"workerType": null,
		"numberOfWorkers": null,
		"maxCapacity": 0.0625,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "publish job.py",
		"scriptLocation": "s3://aws-glue-assets-471112705806-us-east-2/scripts/",
		"language": "python-3.9",
		"spark": false,
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2024-08-31T17:40:19.921Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-471112705806-us-east-2/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": false,
		"observabilityMetrics": false,
		"pythonShellPrebuiltLibraryOption": "analytics",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\r\nimport boto3\r\nfrom datetime import datetime\r\n\r\nQUERY_RESULTS_BUCKET = 's3://athena-bucket-serverless-de-project-cnoord12/'\r\nMY_DATABASE = 'serverless_db_de_project_cnoord12'\r\nSOURCE_PARQUET_TABLE_NAME = 'first_parquet_cta_train_my_first_data_engineering_project_2024_workflow'\r\nNEW_PROD_PARQUET_TABLE_NAME = 'PROD_parquet_cta_train_my_first_data_engineering_project_2024_workflow'\r\nNEW_PROD_PARQUET_TABLE_S3_BUCKET = 's3://published-cta-train-data-parquet-data-engineering'\r\n\r\n# create a string with the current UTC datetime\r\n# convert all special characters to underscores\r\n# this will be used in the table name and in the bucket path in S3 where the table is stored\r\nDATETIME_NOW_INT_STR = str(datetime.now()).replace('-', '_').replace(' ', '_').replace(':', '_').replace('.', '_')\r\n\r\nclient = boto3.client('athena')\r\n\r\n# Refresh the table\r\nqueryStart = client.start_query_execution(\r\n    QueryString = f\"\"\"\r\n    CREATE TABLE {NEW_PROD_PARQUET_TABLE_NAME}_{DATETIME_NOW_INT_STR} WITH\r\n    (external_location='{NEW_PROD_PARQUET_TABLE_S3_BUCKET}/{DATETIME_NOW_INT_STR}/',\r\n    format='PARQUET',\r\n    write_compression='SNAPPY',\r\n    partitioned_by = ARRAY['route'])\r\n    AS\r\n\r\n    SELECT\r\n        *\r\n    FROM \"{MY_DATABASE}\".\"{SOURCE_PARQUET_TABLE_NAME}\"\r\n\r\n    ;\r\n    \"\"\",\r\n    QueryExecutionContext = {\r\n        'Database': f'{MY_DATABASE}'\r\n    }, \r\n    ResultConfiguration = { 'OutputLocation': f'{QUERY_RESULTS_BUCKET}'}\r\n)\r\n\r\n# list of responses\r\nresp = [\"FAILED\", \"SUCCEEDED\", \"CANCELLED\"]\r\n\r\n# get the response\r\nresponse = client.get_query_execution(QueryExecutionId=queryStart[\"QueryExecutionId\"])\r\n\r\n# wait until query finishes\r\nwhile response[\"QueryExecution\"][\"Status\"][\"State\"] not in resp:\r\n    response = client.get_query_execution(QueryExecutionId=queryStart[\"QueryExecutionId\"])\r\n    \r\n# if it fails, exit and give the Athena error message in the logs\r\nif response[\"QueryExecution\"][\"Status\"][\"State\"] == 'FAILED':\r\n    sys.exit(response[\"QueryExecution\"][\"Status\"][\"StateChangeReason\"])"
}